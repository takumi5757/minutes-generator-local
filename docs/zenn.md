---
title: "chatGPTã¨whisperã§è­°äº‹éŒ²ä½œæˆAPIã‚’ä½œã£ã¦ã¿ãŸï¼ˆå‚™å¿˜éŒ²ï¼‰"
emoji: "ğŸ““"
type: "tech"
topics:
  - "ai"
  - "python"
  - "whisper"
  - "chatgpt"
  - "ç”Ÿæˆai"
published: false
published_at: "2024-01-14 17:23"
---

## ã¯ã˜ã‚ã«
æœ¬è¨˜äº‹ã§ã¯ã€è­°äº‹éŒ²ä½œæˆã®è‡ªå‹•åŒ–ã‚’ç›®çš„ã¨ã—ã¦OpenAIã®ChatGPTã¨Whisperã®APIã‚’çµ„ã¿åˆã‚ã›ãŸãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã‚’ç´¹ä»‹ã—ã¾ã™ã€‚ã“ã®ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã§ã¯ã€ã¾ãšä¼šè­°ã®éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’Whisperã§æ–‡å­—èµ·ã“ã—ã—ã€ãã®å¾Œã€ChatGPTã‚’ç”¨ã„ã¦è¦ç´„ã‚’è¡Œã„ã¾ã™ã€‚

ä½¿ç”¨ã—ãŸã‚³ãƒ¼ãƒ‰ã¯ã“ã¡ã‚‰ã§å…±æœ‰ã—ã¦ã„ã¾ã™ï¼š[GitHubãƒªãƒã‚¸ãƒˆãƒª](https://github.com/takumi5757/minutes-generator-local)

## ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼

ãƒ¯ãƒ¼ã‚¯ãƒ•ãƒ­ãƒ¼ã®æ¦‚è¦ã§ã™ã€‚è©³ç´°ã¯å¾Œè¿°ã—ã¾ã™ã€‚


```mermaid
%%{init:{'theme':'neutral'}}%%
graph TD
    A[ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰éŸ³å£°æŠ½å‡º&åœ§ç¸®]
    A --> B[whisperã«æŠ•ã’ã‚‹]
    B --> C[è¿”ã£ã¦ããŸæ–‡å­—èµ·ã“ã—ãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²]
    C --> D[åˆ†å‰²ãƒ†ã‚­ã‚¹ãƒˆæ¯ã«è¦ç´„]
    D --> E[è¦ç´„ã‚’çµåˆ]
    E --> F[ç›®çš„ã«å¿œã˜ã¦chatGPTã«æŠ•ã’ã‚‹]
```

## Whisperå‘¨ã‚Šã®å·¥å¤«

### 25MBã®ãƒ•ã‚¡ã‚¤ãƒ«å®¹é‡ä¸Šé™
Whisper APIã¯25MBã®ãƒ•ã‚¡ã‚¤ãƒ«å®¹é‡ä¸Šé™ãŒã‚ã‚Šã¾ã™ã€‚ãã®ãŸã‚å®¹é‡ã®å¤§ãã„å‹•ç”»ã§ã‚ã‚Œã°éŸ³å£°ã®ã¿ã‚’æŠ½å‡ºã—ã€éŸ³å£°ã®åœ§ç¸®ã‚’ã™ã‚‹ãªã©ã®å¯¾å¿œãŒå¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚
ã§ã¯ã€25MBã¯å…·ä½“çš„ã«ã©ã®ãã‚‰ã„ã®éŒ²éŸ³æ™‚é–“ã«ç›¸å½“ã™ã‚‹ã®ã§ã—ã‚‡ã†ã‹ï¼Ÿ4GéŸ³å£°ã®é›»è©±ã®ãƒ“ãƒƒãƒˆãƒ¬ãƒ¼ãƒˆï¼ˆ12.65kbpsï¼‰ã‚’åŸºæº–ã«ã™ã‚‹ã¨ã€ãŠã‚ˆã4.4æ™‚é–“ã®éŒ²éŸ³ãŒå¯èƒ½ã§ã™ï¼ˆ[å‚è€ƒè¨˜äº‹](https://www.itmedia.co.jp/mobile/articles/1406/05/news159.html)ï¼‰ã€‚ä»Šå›ã®ãƒ‡ãƒ¢ã§ã¯ã€æœ€å¤§4æ™‚é–“ã®éŸ³å£°ãƒ•ã‚¡ã‚¤ãƒ«ã¾ã§ã‚’å…¥åŠ›ã®ä¸Šé™ã¨ã—ã¦ã€25MBä»¥ä¸‹ã«åœ§ç¸®ã—ã¦Whisperã«é€ä¿¡ã—ã¦ã„ã¾ã™ã€‚

subprocessã‚’ä½¿ç”¨ã—ã¦ã€ffmpegã‚’å‘¼ã³å‡ºã—ã¦å¯¾å¿œã—ã¾ã—ãŸã€‚
```python
# å‹•ç”»ã‹ã‚‰éŸ³å£°ã‚’æŠ½å‡ºã™ã‚‹
    def extract_audio_from_video(
        input_tempfile: tempfile.NamedTemporaryFile,
    ) -> tempfile.NamedTemporaryFile:
        # éŸ³å£°ã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã®ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä½œæˆ
        with tempfile.NamedTemporaryFile(
            delete=False, suffix=".mp3"
        ) as output_tempfile:
            subprocess.run(
                [
                    "ffmpeg",
                    "-y",
                    "-i",
                    input_tempfile.name,
                    "-q:a",
                    "0",
                    "-map",
                    "a",
                    output_tempfile.name,
                ]
            )
        os.remove(input_tempfile.name)
        return output_tempfile
```

```python
# éŸ³å£°ã‚’åœ§ç¸®ã™ã‚‹
    @classmethod
    def compress_audio(
        cls,
        input_file: tempfile.NamedTemporaryFile,
    ) -> tempfile.NamedTemporaryFile:
        logger.info("=== compress audio ===")

        # Check audio duration
        duration = cls.get_audio_duration(input_file.name)

        # Calculate bitrate based on audio duration
        bitrate = cls.calculate_bitrate(duration)
        logger.info(f"Target bitrate: {bitrate}")

        with tempfile.NamedTemporaryFile(
            delete=False, suffix=".mp3"
        ) as output_tempfile:
            subprocess.run(
                [
                    "ffmpeg",
                    "-y",
                    "-i",
                    input_file.name,
                    "-codec:a",
                    "mp3",
                    "-ar",
                    "16000",
                    "-ac",
                    "1",
                    "-b:a",
                    bitrate,
                    output_tempfile.name,
                ]
            )

        os.remove(input_file.name)
        logger.info(f"Compressed audio size:{os.path.getsize(output_tempfile.name)}")
        return output_tempfile
```

### ç„¡éŸ³åŒºé–“ã®æ‰±ã„
ç„¡éŸ³åŒºé–“ã‚’å‰Šé™¤ã—ãªã„ã¨ã€ç„¡éŸ³åŒºé–“ã§ä»¥ä¸‹ã®ã‚ˆã†ãªãƒªãƒ”ãƒ¼ãƒ†ã‚£ãƒ³ã‚°ãŒèµ·ãã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚
```
ã€œã¨ã„ã†ã“ã¨ã§ã™ã€‚ãã‚Œã§ã€ãã‚Œã§ã€ãã‚Œã§ã€ãã‚Œã§ã€ãã‚Œã§ã€ãã‚Œã§ã€ãã‚Œã§ã€ãã‚Œã§ã€
```
ä»¥ä¸‹ã®ãƒªãƒã‚¸ãƒˆãƒªã‚’å‚è€ƒã«ã€ç„¡éŸ³åŒºé–“ã‚’å‰Šé™¤ã™ã‚‹å‡¦ç†ã‚’å®Ÿè£…ã—ã¾ã—ãŸã€‚
https://github.com/snakers4/silero-vad


## ChatGPTå‘¨ã‚Šã®å·¥å¤«

### max_tokenå¯¾å¿œ
ChatGPTã«ã¯ã€ä¸€åº¦ã«å‡¦ç†ã§ãã‚‹ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«åˆ¶é™ãŒã‚ã‚Šã¾ã™ã€‚ãã®ãŸã‚ã€é•·æ–‡ã®è¦ç´„ã«ã¯æ–‡ç« ã®åˆ†å‰²ãŒå¿…è¦ã§ã™ã€‚åˆ†å‰²ã®æ–¹æ³•ã¨ã—ã¦ã€ã€ŒMap Reduceã€ã€ã€ŒMap Rerankã€ã€ã€ŒRefineã€ãªã©ãŒã‚ã‚Šã¾ã™ãŒã€ä»Šå›ã¯ã€ŒMap Reduceã€ã‚’ä½¿ç”¨ã—ã¾ã—ãŸã€‚ã“ã‚Œã¯ã€åˆ†å‰²ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã”ã¨ã«å€‹åˆ¥ã®è¦ç´„ã‚’ç”Ÿæˆã—ã€ãã‚Œã‚‰ã‚’1ã¤ã®è¦ç´„ã«çµ±åˆã™ã‚‹æ‰‹æ³•ã§ã™ï¼ˆ[å‚è€ƒè³‡æ–™](https://python.langchain.com/docs/modules/chains/document/)ï¼‰ã€‚

```python
    @classmethod
    def map_sammaries(cls, text: str):
        text_splitter = CharacterTextSplitter.from_tiktoken_encoder(
            separator=" ", chunk_size=cls.CHUNK_SIZE, chunk_overlap=cls.chunk_overlap
        )
        text_chunks = text_splitter.split_text(text)
        total_tokens = 0
        prompt_tokens = 0
        completion_tokens = 0
        costs = 0
        response_messages = []

        for chunk in text_chunks:
            logger.info(chunk)

            messages = [
                {
                    "role": "system",
                    "content": """
                        ã‚ãªãŸã¯ä¼šè­°ã®è­°äº‹éŒ²ã‚’ä½œæˆã™ã‚‹ãƒ—ãƒ­ãƒ•ã‚§ãƒƒã‚·ãƒ§ãƒŠãƒ«ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
                        ã“ã‚Œã‹ã‚‰ä¼šè­°ã®æ–‡å­—èµ·ã“ã—ã—ãŸãƒ†ã‚­ã‚¹ãƒˆã‚’åˆ†å‰²ã—ã¦æ¸¡ã—ã¾ã™ã€‚
                        ãƒ†ã‚­ã‚¹ãƒˆã¯è©±è€…åˆ†é›¢ã‚’ã—ã¦ã„ã¾ã›ã‚“ã€‚
                        ã“ã®æ–‡ç« ã‹ã‚‰é‡è¦ãªå†…å®¹ã‚’æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
                        ã‚ãªãŸã®æ¨å¯Ÿã¯ã›ãšã€æ–‡ç« ã«æ˜è¨˜ã•ã‚Œã¦ã„ã‚‹å†…å®¹ã‚’ãã®ã¾ã¾æŠ½å‡ºã—ã¦ãã ã•ã„ã€‚
                        æŠ½å‡ºã¯ç®‡æ¡æ›¸ãã§ã¯ãªãã€æ–‡ç« ã§è¡Œãªã£ã¦ãã ã•ã„ã€‚
                        """,
                },
                {"role": "user", "content": chunk},
            ]
            num_tokens = num_tokens_from_messages(messages, model=cls.MODEL)
            response = openai.ChatCompletion.create(
                model=cls.MODEL,
                messages=messages,
                temperature=0,
                max_tokens=min(
                    cls.MAX_TOKENS // len(text_chunks), cls.MAX_TOKENS - num_tokens
                ),
            )
            total_tokens += response["usage"]["total_tokens"]
            prompt_tokens += response["usage"]["prompt_tokens"]
            completion_tokens += response["usage"]["completion_tokens"]
            costs += (
                response["usage"]["prompt_tokens"] * cls.COST_DICT[cls.MODEL]["input"]
                + response["usage"]["completion_tokens"]
                * cls.COST_DICT[cls.MODEL]["output"]
            ) / 1000
            response_messages.append(response["choices"][0]["message"]["content"])
            logger.info(f"chat create: {response['choices'][0]['message']['content']}")
        return response_messages, costs
```

### å‡ºåŠ›ã®æ§‹é€ åŒ–
function_callingã§ã€pydanticã®BaseModelã®schemaã‚’æ¸¡ã—ã¦ã‚ã’ã‚‹ã“ã¨ã§å‡ºåŠ›ã®æ§‹é€ åŒ–ãŒã§ãã¾ã™ã€‚

```python
# å‡ºåŠ›ã®ãƒ¢ãƒ‡ãƒ«ã®å®šç¾©
class SimpleSummary(BaseModel):
    summary: str = Field(..., description="è¦ç´„ã®å†…å®¹ã‚’æ–‡ç« ã§æ›¸ã„ãŸã‚‚ã®")
    summary_bullet: List[str] = Field(..., description="è¦ç‚¹ã®ãƒªã‚¹ãƒˆ")
    decisions: List[str] = Field(..., description="ã‚¿ã‚¹ã‚¯ä»¥å¤–ã§æ±ºå®šã•ã‚ŒãŸäº‹é …ãƒªã‚¹ãƒˆ")
    tasks: List[str] = Field(..., description="ã‚„ã‚‹ã¹ãã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆ")

~~~
    @classmethod
    def get_simple_summary(cls, doc_summaries: str):
        costs = 0

        template = """
        ã‚ãªãŸã¯ä¼šè­°ã®è­°äº‹éŒ²ã‚’ä½œæˆã™ã‚‹ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚
        ã“ã‚Œã‹ã‚‰ä¼šè­°ã®æ–‡å­—èµ·ã“ã—ã®è¦ç‚¹ã‚’æŠ½å‡ºã—ãŸæ–‡ç« ã‚’æ¸¡ã—ã¾ã™ã€‚
        ã“ã®ä¼šè­°ã®è¦ç´„ã€è¦ç‚¹ã®ãƒªã‚¹ãƒˆã€æ±ºå®šäº‹é …ã®ãƒªã‚¹ãƒˆã€ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆã‚’è¿”ã—ã¦ãã ã•ã„ã€‚
        ä¼šè­°ã®è¦ç´„ã¯æ¸¡ã—ãŸæ–‡ç« ã®ä½“è£ã‚’ç•™ã‚ã‚‹ç¨‹åº¦ã§æ¼ã‚Œã®ãªã„æ–‡ç« ã§æ›¸ã„ã¦ãã ã•ã„ã€‚ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆã«ã¯ä»Šå¾Œã‚„ã‚‹ã¹ãã‚¿ã‚¹ã‚¯ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚
        ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆã«ã¯æ—¢ã«å®Œäº†ã—ãŸã“ã¨ã«ã¤ã„ã¦ã¯è¨˜è¼‰ã—ãªã„ã‚ˆã†ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚
        æ±ºå®šäº‹é …ã®ãƒªã‚¹ãƒˆã«ã¯ã‚¿ã‚¹ã‚¯ä»¥å¤–ã§æ±ºå®šã•ã‚ŒãŸäº‹é …ã‚’æ›¸ã„ã¦ãã ã•ã„ã€‚

        ä»¥ä¸‹ã¯è¦ç´„ã®ã‚»ãƒƒãƒˆã§ã‚ã‚‹ï¼š
        {doc_summaries}"""

        prompt = PromptTemplate(template=template, input_variables=["doc_summaries"])
        messages = [
            {"role": "user", "content": prompt.format(doc_summaries=doc_summaries)},
        ]
        functions = [
            {
                "name": "get_simple_summary",
                "description": """ä¼šè­°ã®æ–‡å­—èµ·ã“ã—ã®è¦ç‚¹ãªã©ã‚’æŠ½å‡ºã—ãŸæ–‡ç« ã‹ã‚‰ä¼šè­°ã®è¦ç´„ã€ä¼šè­°ã®è¦ç‚¹ã®ãƒªã‚¹ãƒˆã€
                    ä¼šè­°ã§æ±ºå®šã•ã‚ŒãŸäº‹é …ã®ãƒªã‚¹ãƒˆã€ã‚¿ã‚¹ã‚¯ã®ãƒªã‚¹ãƒˆã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã®å‡¦ç†ã§ã™ã€‚""",
                "parameters": SimpleSummary.schema(),# å‡ºåŠ›ã®ãƒ¢ãƒ‡ãƒ«ã®ã‚¹ã‚­ãƒ¼ãƒã‚’æ¸¡ã™
            }
        ]
        message_tokens = num_tokens_from_messages(messages, model=cls.MODEL)
        functions_tokens = num_tokens_from_functions(functions, model=cls.MODEL)
        logger.info(f"message_tokens: {message_tokens}")
        logger.info(f"functions_tokens: {functions_tokens}")
	
        response = openai.ChatCompletion.create(
            model=cls.MODEL,
            messages=messages,
            functions=functions,
            function_call={"name": "get_simple_summary"},
            temperature=0,
            max_tokens=max(
                cls.MAX_TOKENS - message_tokens - functions_tokens, 0
            ),  # tokenã‚’ã‚«ã‚¦ãƒ³ãƒˆã—ã¦è£œæ­£ã™ã‚‹
        )

        costs += (
            response["usage"]["prompt_tokens"] * cls.COST_DICT[cls.MODEL]["input"]
            + response["usage"]["completion_tokens"]
            * cls.COST_DICT[cls.MODEL]["output"]
        ) / 1000

        return response, costs
	
~~~
# å‡ºåŠ›ã®å–ã‚Šå‡ºã—
response_message = response["choices"][0]["message"]
# Step 2: check if GPT wanted to call a function
if response_message.get("function_call"):
# Step 3: call the function
# Note: the JSON response may not always be valid; be sure to handle errors
try:
    function_args = json.loads(response_message["function_call"]["arguments"])
except Exception as e:
    logger.error(f"Error parsing function arguments: {e}")
    function_args = None
simple_summary = function_args
else:
simple_summary = None
```
ã“ã‚Œã§ã€simple_summaryã«å‡ºåŠ›ãƒ¢ãƒ‡ãƒ«ã®ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’ã‚­ãƒ¼ã«ã—ãŸdictãŒæ ¼ç´ã•ã‚Œã¾ã™ã€‚

## ã‚³ã‚¹ãƒˆï¼ˆ2024/01/14æ™‚ç‚¹ï¼‰

### whisper

$0.006 / 1åˆ†

### gpt-4-turbo

| Model              | Input             | Output            |
| ------------------ | ----------------- | ----------------- |
| gpt-4-1106-preview | $0.01 / 1K tokens | $0.03 / 1K tokens |

### GPT-4

| Model       | Input             | Output            |
| ----------- | ----------------- | ----------------- |
| 8K context  | $0.03 / 1K tokens | $0.06 / 1K tokens |
| 32K context | $0.06 / 1K tokens | $0.12 / 1K tokens |

æ—¥æœ¬èªã ã¨1~0.9æ–‡å­—/tokenç¨‹åº¦

### ****GPT-3.5 Turbo****

| Model       | Input               | Output              |
| ----------- | ------------------- | ------------------- |
| 16K context | $0.0010 / 1K tokens | $0.0020 / 1K tokens |

gpt-4-turboå®‰ã„ã§ã™ã­ã€ä»Šå›ã®è©¦ä½œæ™‚ã«ã¯ã¾ã ç„¡ã‹ã£ãŸãƒ¢ãƒ‡ãƒ«ãªã®ã§è©¦ã›ã¦ã¯ãªã„ã§ã™ã€‚

ã¡ãªã¿ã«æ—¥æœ¬èªã§ã¯ã€ç´„1ï½0.9æ–‡å­—ãŒ1ãƒˆãƒ¼ã‚¯ãƒ³ã«ç›¸å½“ã—ã¾ã™ã€‚

30åˆ†ãã‚‰ã„ã®å¯¾è«‡å‹•ç”»ã ã¨ã€gpt-4(8k)ã§è¦ç´„ã‚³ã‚¹ãƒˆã¯
Whisper cost: 0.16 $
GPT cost: 0.57 $
Total cost: 0.73 $
ã§1ãƒ‰ãƒ«ä»¥ä¸‹ã§ã™ã€å®‰ã„ã§ã™ã­ã€‚

## èª²é¡Œã¨æ”¹å–„ç‚¹

### Whisperã®èª²é¡Œ

- äººåã®èª¤å¤‰æ›ã‚„è¡¨è¨˜ã®æºã‚ŒãŒç”Ÿã˜ã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™
  - ä¸€å¿œWhisperã«ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã™ã‚‹ã“ã¨ã§å¯¾ç­–ãŒã‚ã‚Šã¾ã™ã€‚
- è©±è€…ã®åŒºåˆ¥
  - Whisperã¨Pyannoteã‚’ä½¿ç”¨ã—ãŸè©±è€…åˆ†é›¢æ–¹æ³•ã‚„ã€Azure Open AI Serviceã§ã®å¯¾å¿œç­–ãŒã‚ã‚Šã¾ã™ã€‚

[Whisperã¨Pyannoteã‚’ç”¨ã„ãŸè©±è€…åˆ†é›¢ã¨éŸ³å£°èªè­˜ | Hakky Handbook](https://book.st-hakky.com/docs/whisper-pyannote-diarization/)

@[tweet](https://twitter.com/daiki15036604/status/1702828819558912132)



### ChatGPTã®èª²é¡Œ

- ã€Œè¡Œã£ãŸã€ã€ã€Œã“ã‚Œã‹ã‚‰è¡Œã†ã€ã¨ã„ã£ãŸè¡Œå‹•ã®åŒºåˆ¥ã®èª¤ã‚Š
- é›‘è«‡ã¨æ¥­å‹™å†…å®¹ã®åŒºåˆ¥ãŒæ›–æ˜§ã«ãªã‚‹ã“ã¨ãŒã‚ã‚‹

## ã¾ã¨ã‚
è­°äº‹éŒ²ã®ç”Ÿæˆã‚’è‡ªå‹•åŒ–ã™ã‚‹ãŸã‚ã«OpenAIã®ChatGPTã¨Whisperã®APIã‚’çµ„ã¿åˆã‚ã›ã¾ã—ãŸã€‚ä¸»ãªãƒ•ãƒ­ãƒ¼ã¯ã€ä¼šè­°ã®éŒ²éŸ³ãƒ‡ãƒ¼ã‚¿ã‚’Whisperã§æ–‡å­—èµ·ã“ã—ã—ã€ãã®å¾ŒChatGPTã§è¦ç´„ã™ã‚‹ã¨ã„ã†ã‚‚ã®ã§ã™ã€‚é•·æ™‚é–“ã®ä¼šè­°éŸ³å£°ãªã©ã§ã‚‚æ•°åˆ†ã§è­°äº‹éŒ²ãŒã§ãã‚‹ç‚¹ãŒé­…åŠ›çš„ã§ã™ã€‚

ã¾ãŸã€ä»Šå›ã¯è¦ç´„ã‚’ç›®çš„ã¨ã—ã¦æ‰±ã£ã¦ãã¾ã—ãŸãŒã€è¦ç´„ä»¥å¤–ã®ã‚¿ã‚¹ã‚¯ã§ã‚‚æ´»ç”¨ã§ãã‚‹ç¯„å›²ã¯ã¨ã¦ã‚‚åºƒã„ã¨è€ƒãˆã¦ã„ã¾ã™ã€‚
ä»Šå¾Œã€å¾—ã‚‰ã‚ŒãŸçŸ¥è¦‹ã‚’åŸºã«ã€ã•ã¾ã–ã¾ãªç”¨é€”ã¸ã®é©ç”¨ã‚‚æ¨¡ç´¢ã—ã¦ã„ãäºˆå®šã§ã™ã€‚

â€»å¾Œå­¦ã®ãŸã‚ã€èª¤ã‚Šã‚’è¦‹ã¤ã‘ãŸå ´åˆã¯ã‚³ãƒ¡ãƒ³ãƒˆæ¬„ãªã©ã§ãŠçŸ¥ã‚‰ã›ã„ãŸã ã‘ã‚‹ã¨æœ‰é›£ã„ã§ã™ã€‚